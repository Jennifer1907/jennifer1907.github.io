<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Data Scientist & ML Engineer ¬∑ Turning data into impact">
<meta name="author" content="Jen (Ha) Nguyen">

<!-- Open Graph -->
<meta property="og:title" content="7 A/B Testing Mistakes That Kill Your Experiments">
<meta property="og:description" content="Data Scientist & ML Engineer ¬∑ Turning data into impact">
<meta property="og:type" content="website">
<meta property="og:url" content="http://localhost:4001/blog/2025/09/18/ab-testing-mistakes/">

<!-- Twitter -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="7 A/B Testing Mistakes That Kill Your Experiments">

<title>7 A/B Testing Mistakes That Kill Your Experiments ¬∑ Jen (Ha) Nguyen</title>

<!-- Fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Outfit:wght@500;700;800&display=swap" rel="stylesheet">

<!-- Icons (Lucide via CDN) -->
<script src="https://unpkg.com/lucide@latest/dist/umd/lucide.min.js"></script>

<!-- Styles -->
<link rel="stylesheet" href="/assets/css/main.css">

</head>
<body>
  <nav class="nav" role="navigation" aria-label="Main navigation">
  <a href="/" class="nav__logo">
    Jennifer<span>.</span>
  </a>

  <ul class="nav__links">
    <li><a href="/">Home</a></li>
    <li><a href="/resume/">Resume</a></li>
    <!-- <li><a href="/projects/">Projects</a></li> -->
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/awards/">Awards</a></li>
    <li><a href="/contact/" class="nav__cta">Contact</a></li>
  </ul>

  <button class="nav__hamburger" aria-label="Open menu" aria-expanded="false">
    <span></span><span></span><span></span>
  </button>
</nav>

<!-- Mobile overlay -->
<div class="nav__mobile-menu" role="dialog" aria-label="Mobile navigation">
  <button class="nav__mobile-close" aria-label="Close menu">‚úï</button>
  <a href="/">Home</a>
  <a href="/resume/">Resume</a>
  <!-- <a href="/projects/">Projects</a> -->
  <a href="/blog/">Blog</a>
  <a href="/awards/">Awards</a>
  <a href="/contact/">Contact</a>
</div>


  <main>
    <article>
  <header class="post-hero">
    <div class="container--narrow">
      <div class="post-hero__meta">
        <a href="/blog/" style="color: var(--accent-light)">‚Üê Blog</a>
        &nbsp;¬∑&nbsp;
        <span>September 18, 2025</span>
        &nbsp;¬∑&nbsp;
        <span>7 min read</span>
        
        &nbsp;¬∑&nbsp;
        <span class="tag tag--accent">analytics</span>
        
      </div>
      <h1>7 A/B Testing Mistakes That Kill Your Experiments</h1>
      
      <p class="post-hero__excerpt">After running hundreds of experiments at Shopify, I've seen the same mistakes sink otherwise promising A/B tests. Here's how to avoid them.</p>
      
      <div style="display:flex;gap:.75rem;flex-wrap:wrap;margin-top:1.5rem;">
        
        <span class="tag tag--muted">A/B Testing</span>
        
        <span class="tag tag--muted">Statistics</span>
        
        <span class="tag tag--muted">Analytics</span>
        
      </div>
    </div>
  </header>

  <div class="post-content">
    <div class="container--narrow">
      <p>After running hundreds of experiments at Shopify‚Äîand reviewing dozens more from engineers and PMs who wanted a second opinion‚ÄîI‚Äôve seen the same mistakes over and over. Here are the seven that cause the most damage, and how to fix them.</p>

<h2 id="1-peeking-at-results-and-stopping-early">1. Peeking at Results and Stopping Early</h2>

<p><strong>The mistake</strong>: The experiment has been running for 3 days. You check the dashboard and see p=0.04. You call it significant and ship.</p>

<p><strong>The problem</strong>: The p-value fluctuates. If you check 10 times and stop when it crosses 0.05, your false positive rate skyrockets to ~40%.</p>

<p><strong>The fix</strong>: Pre-commit to a sample size based on a power analysis <em>before</em> you start. Then don‚Äôt stop early‚Äîor use sequential testing methods (like mSPRT or always-valid confidence intervals) that are designed for continuous monitoring.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">statsmodels.stats.power</span> <span class="kn">import</span> <span class="n">NormalIndPower</span>

<span class="n">analysis</span> <span class="o">=</span> <span class="n">NormalIndPower</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">analysis</span><span class="p">.</span><span class="n">solve_power</span><span class="p">(</span>
    <span class="n">effect_size</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>    <span class="c1"># minimum detectable effect (as proportion diff / pooled std)
</span>    <span class="n">power</span><span class="o">=</span><span class="mf">0.80</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">alternative</span><span class="o">=</span><span class="s">"two-sided"</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Required sample size per variant: </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="2-multiple-comparisons-without-correction">2. Multiple Comparisons Without Correction</h2>

<p><strong>The mistake</strong>: You test 5 button colours. One comes out at p=0.03. You ship the winner.</p>

<p><strong>The problem</strong>: With 5 comparisons at Œ±=0.05, the probability of at least one false positive is <code class="language-plaintext highlighter-rouge">1 - (1-0.05)^5 ‚âà 23%</code>. You‚Äôre likely shipping noise.</p>

<p><strong>The fix</strong>: Apply a correction. Bonferroni is conservative but simple:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">statsmodels.stats.multitest</span> <span class="kn">import</span> <span class="n">multipletests</span>

<span class="n">p_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.22</span><span class="p">]</span>
<span class="n">reject</span><span class="p">,</span> <span class="n">corrected_p</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">multipletests</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">"bonferroni"</span><span class="p">)</span>
</code></pre></div></div>

<p>Or use Benjamini-Hochberg (<code class="language-plaintext highlighter-rouge">method="fdr_bh"</code>) if you care more about controlling false discovery rate than family-wise error rate.</p>

<h2 id="3-ignoring-metric-variance">3. Ignoring Metric Variance</h2>

<p><strong>The mistake</strong>: You run a revenue-per-user test. Control: $12.40. Treatment: $13.10. Lift: 5.6%. ‚ÄúStatistical significance!‚Äù üéâ</p>

<p><strong>The problem</strong>: Revenue is <em>highly</em> skewed‚Äîa few big spenders drive enormous variance. Standard t-tests assume roughly normal distributions. With highly skewed data, your standard errors are wrong.</p>

<p><strong>The fix</strong>: Use bootstrap confidence intervals, or apply a variance reduction technique like CUPED.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">bootstrap_mean_diff</span><span class="p">(</span><span class="n">control</span><span class="p">,</span> <span class="n">treatment</span><span class="p">,</span> <span class="n">n_bootstrap</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">diffs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_bootstrap</span><span class="p">):</span>
        <span class="n">c_sample</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">control</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">control</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">t_sample</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">treatment</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">treatment</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">diffs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_sample</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">c_sample</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">diffs</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span>

<span class="n">ci_low</span><span class="p">,</span> <span class="n">ci_high</span> <span class="o">=</span> <span class="n">bootstrap_mean_diff</span><span class="p">(</span><span class="n">control_revenue</span><span class="p">,</span> <span class="n">treatment_revenue</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"95% CI for lift: [</span><span class="si">{</span><span class="n">ci_low</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">ci_high</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">]"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="4-network-effects-and-sutva-violations">4. Network Effects and SUTVA Violations</h2>

<p><strong>The mistake</strong>: You A/B test a social feature by randomising individual users. Treatment users can interact with control users.</p>

<p><strong>The problem</strong>: The Stable Unit Treatment Value Assumption (SUTVA) is violated‚Äîtreatment affects control users through spillover. Your estimated effect is biased.</p>

<p><strong>The fix</strong>: Use cluster-level randomisation (e.g., randomise by geographic region, social graph component, or organisation) when your intervention has network effects.</p>

<h2 id="5-not-checking-guardrail-metrics">5. Not Checking Guardrail Metrics</h2>

<p><strong>The mistake</strong>: Checkout conversion improved 8%. Ship it!</p>

<p><strong>The problem</strong>: You didn‚Äôt notice that returns went up 15% and customer support tickets doubled. Your north-star metric went up; your business went down.</p>

<p><strong>The fix</strong>: Define guardrail metrics upfront‚Äîmetrics that must <em>not</em> degrade, even if your primary metric improves. If a guardrail is violated, the experiment fails regardless of the primary result.</p>

<p>At Shopify we tracked: conversion rate (primary), returns rate, support ticket rate, and load time (guardrails).</p>

<h2 id="6-segment-only-reporting">6. Segment-Only Reporting</h2>

<p><strong>The mistake</strong>: The overall experiment was neutral, but in the mobile segment, treatment was +12%. ‚ÄúLet‚Äôs ship just for mobile users!‚Äù</p>

<p><strong>The problem</strong>: This is HARKing (Hypothesising After Results are Known). If you look at 20 segments, you‚Äôll find a winner by chance in at least one. This finding hasn‚Äôt been validated.</p>

<p><strong>The fix</strong>: Pre-register your subgroup analyses. If you do post-hoc segment analysis, treat it as hypothesis generation‚Äîdesign a new, properly powered experiment to confirm the finding.</p>

<h2 id="7-novelty-effects">7. Novelty Effects</h2>

<p><strong>The mistake</strong>: Users love the new feature! Engagement is up 25% in the first week. You ship.</p>

<p><strong>The problem</strong>: Users are engaging because it‚Äôs <em>new</em>, not because it‚Äôs <em>better</em>. After 2‚Äì3 weeks, engagement often regresses back to baseline or below.</p>

<p><strong>The fix</strong>: Run your experiments for at least 2 full weeks, and ideally long enough to observe a stable post-novelty steady state. Plot engagement curves over time‚Äîif treatment is still elevated after 14 days, you can be more confident.</p>

<hr />

<h2 id="quick-checklist-before-launching-any-ab-test">Quick Checklist Before Launching Any A/B Test</h2>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Sample size calculated via power analysis?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Runtime set before launch (not after peeking)?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Randomisation unit is the right unit (user vs session vs cluster)?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Guardrail metrics defined?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Multiple comparisons strategy defined (if testing multiple variants/metrics)?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Subgroup analyses pre-registered?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Novelty effect considered in planned duration?</li>
</ul>

<p>Getting experimentation right is one of the highest-leverage skills in data analytics. The statistics are learnable; the discipline takes practice. Good luck!</p>


      <nav class="post-nav">
        
        <a href="/blog/2025/08/22/career-advice-data-analyst/">
          <div class="post-nav__dir">‚Üê Previous</div>
          <div class="post-nav__title">How I Went From Intern to Senior Data Analyst in 5 Years</div>
        </a>
        
        
        <a href="/blog/2025/10/05/churn-prediction-guide/" style="text-align:right">
          <div class="post-nav__dir">Next ‚Üí</div>
          <div class="post-nav__title">Building a Production Churn Prediction Model: End-to-End</div>
        </a>
        
      </nav>
    </div>
  </div>
</article>

  </main>

  <footer class="footer">
  <div class="footer__socials">
    <a href="https://linkedin.com/in/ha-nguyen" class="footer__social" target="_blank" rel="noopener" aria-label="LinkedIn">
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
    </a>
    <a href="https://github.com/jennifer1907" class="footer__social" target="_blank" rel="noopener" aria-label="GitHub">
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
    </a>
    <a href="https://kaggle.com/jenniferh" class="footer__social" target="_blank" rel="noopener" aria-label="Kaggle">
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.333z"/></svg>
    </a>
    <a href="mailto:hanguyen.aiovn@gmail.com" class="footer__social" aria-label="Email">
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
    </a>
  </div>

  <ul class="footer__links">
    <li><a href="/">Home</a></li>
    <li><a href="/resume/">Resume</a></li>
    <li><a href="/projects/">Projects</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/awards/">Awards</a></li>
  </ul>

  <p class="footer__copy">
    ¬© 2026 Jen (Ha) Nguyen ¬∑ Built with Jekyll &amp; ‚ô•
  </p>
</footer>


  <script src="/assets/js/main.js"></script>
  <script>if (window.lucide) lucide.createIcons();</script>
</body>
</html>
